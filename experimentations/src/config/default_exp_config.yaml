experiment:
    name: 'RotEquivariance'
    sub-experiment: 'default'
    sub-experiment-id: 0
    tags: {}


model:
    backbone: 'unet'
    steered: 'all'
        # steering: 'attention' # 'attention' / 'angle' / 'vec' / 'vec_norm'
        # rho_nonlinearity: None # None / 'tanh' / 'normalized'
        # base: None # kernel_size / {k: [r0, r1], ...} / {'kr': ..., 'max_k': k, 'phase': 0}
        # attention_mode: 'shared' # 'shared' / 'feature' / False
        # attention_base: ...
    nfeatures: 16
    nscale: 5
    depth: 2
    kernel: 5
    padding: 'auto'
    downsampling: 'maxpooling'
    upsampling: 'conv'
    batchnorm: True


training:
    seed: 1234
    dataset-file: 'vessels.h5'
    use-preprocessing: True
    dataset-path: 'default'
    training-dataset: 'DRIVE'
    training-dataset-factor: 8
    max-epoch: 30
    val-every-n-epoch: 1
    half-precision: False
    optimize: 'val-acc'
    num-worker: 6
    early-stopping:
        monitor: 'train-loss'
        min_delta: 0.0005
        patience: 10
        mode: 'min'

    
hyper-parameters:
    lr: 1.e-2
    batch-size: 8
    accumulate-gradient-batch: 1
    drop-out: 0
    loss: 'binaryCE'  # 'focalLoss' / 'binaryCE' / 'dice'
    smooth-label: 0
    optimizer:
        type: 'Adam' # also supported: AdamW and Adammax
        weight_decay: 0
        beta: 0.9       # coefficients used for computing running averages of gradient
        beta_sqr: 0.999 # and its square
        amsgrad: True

        lr-decay-factor: 0

    # optimizer:
    #     type: 'ASGD' 
    #     weight_decay: 0
    #    alpha: 0.75
    #    t0: 1000000.0
    
    # optimizer:
    #     type: 'SGD' 
    #     weight_decay: 0
    #    momentum: 0,
    #    dampening: 0
    #    nesterov: False

    #lr-scheduler:
    #    type: 'reduceOnPlateau'
    #


data-augmentation:
    rotation: True
    elastic: True
    crop-size: 758
    elastic-transform:
        alpha: 10
        sigma: 20
        alpha-affine: 50
